{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6816442,"sourceType":"datasetVersion","datasetId":3920741}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-29T12:46:16.800816Z","iopub.execute_input":"2023-10-29T12:46:16.801304Z","iopub.status.idle":"2023-10-29T12:46:17.182776Z","shell.execute_reply.started":"2023-10-29T12:46:16.801262Z","shell.execute_reply":"2023-10-29T12:46:17.181865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2= pd.read_csv('/kaggle/input/102-ml-hw2-data/train/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:46:17.184799Z","iopub.execute_input":"2023-10-29T12:46:17.185398Z","iopub.status.idle":"2023-10-29T12:46:17.309915Z","shell.execute_reply.started":"2023-10-29T12:46:17.185362Z","shell.execute_reply":"2023-10-29T12:46:17.308732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data2.info())","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:46:17.311621Z","iopub.execute_input":"2023-10-29T12:46:17.312005Z","iopub.status.idle":"2023-10-29T12:46:17.344763Z","shell.execute_reply.started":"2023-10-29T12:46:17.311950Z","shell.execute_reply":"2023-10-29T12:46:17.343706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data2.copy()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:46:17.347739Z","iopub.execute_input":"2023-10-29T12:46:17.348146Z","iopub.status.idle":"2023-10-29T12:46:17.354028Z","shell.execute_reply.started":"2023-10-29T12:46:17.348104Z","shell.execute_reply":"2023-10-29T12:46:17.353141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install nltk\n!pip3 install symspellpy\n!pip3 install pandas","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:46:17.355300Z","iopub.execute_input":"2023-10-29T12:46:17.356109Z","iopub.status.idle":"2023-10-29T12:47:13.445677Z","shell.execute_reply.started":"2023-10-29T12:46:17.356080Z","shell.execute_reply":"2023-10-29T12:47:13.444497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef pbx_into_sp(org_str) : \n    org_str = re.sub('\\W+', ' ', org_str).replace(\"_\", '').strip()#將句子中的特殊符號以空格取代\n    return org_str\n# 建立新欄位'clear_comment'存放清理好的評論\ndata['clear_text'] = data['text'].apply(lambda x: pbx_into_sp(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:13.447270Z","iopub.execute_input":"2023-10-29T12:47:13.447664Z","iopub.status.idle":"2023-10-29T12:47:13.781433Z","shell.execute_reply.started":"2023-10-29T12:47:13.447632Z","shell.execute_reply":"2023-10-29T12:47:13.780448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:13.782733Z","iopub.execute_input":"2023-10-29T12:47:13.783039Z","iopub.status.idle":"2023-10-29T12:47:13.794466Z","shell.execute_reply.started":"2023-10-29T12:47:13.783014Z","shell.execute_reply":"2023-10-29T12:47:13.793628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\n# 進行停用詞過濾\n# 下載NLTK的停用詞列表\nnltk.download('punkt')  #用于下载NLTK的标点符号工具\nnltk.download('stopwords') #用于下载英语停用词列表\nnltk_stopwords = nltk.corpus.stopwords.words('english')\n\ndef rmsw_function(text, stopword_list):\n    return ' '.join([word for word in word_tokenize(text) if word not in stopword_list])\n    #这个过程的目标是从文本中去除那些在文本分类或自然语言处理任务中通常没有信息量的常见词汇，\n    #例如 \"the\"、\"and\"、\"in\" 等。这可以帮助模型更好地关注文本中的重要内容。\n\n# 使用前面初步清理過的clear_comment來過濾停用詞\n# 建立新欄位'clear_comment_sw'存放過濾停用詞的評論\ndata['clear_text_sw'] = data['clear_text'].apply(lambda x: rmsw_function(x, nltk_stopwords))","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:13.795598Z","iopub.execute_input":"2023-10-29T12:47:13.796373Z","iopub.status.idle":"2023-10-29T12:47:21.679904Z","shell.execute_reply.started":"2023-10-29T12:47:13.796346Z","shell.execute_reply":"2023-10-29T12:47:21.678948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:21.681340Z","iopub.execute_input":"2023-10-29T12:47:21.681878Z","iopub.status.idle":"2023-10-29T12:47:21.690235Z","shell.execute_reply.started":"2023-10-29T12:47:21.681821Z","shell.execute_reply":"2023-10-29T12:47:21.689229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#文本長度，即文本總字數。\ndef get_txt_len(txt):\n    txt_list = txt.split(' ')\n    return len(txt_list)\n    \ndata['txt_len'] = data['clear_text_sw'].apply(lambda x: get_txt_len(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:21.693828Z","iopub.execute_input":"2023-10-29T12:47:21.694199Z","iopub.status.idle":"2023-10-29T12:47:21.743176Z","shell.execute_reply.started":"2023-10-29T12:47:21.694171Z","shell.execute_reply":"2023-10-29T12:47:21.742109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#文本中的词汇频率和词汇量（不重复的词汇数量）\ndef get_frequency(txt):\n    words = nltk.word_tokenize(txt)\n    word_frequency = nltk.FreqDist(words)\n    vocabulary_size = len(word_frequency)\n    return vocabulary_size\n    \ndata['word_frequency'] = data['clear_text_sw'].apply(lambda x: get_frequency(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:21.744449Z","iopub.execute_input":"2023-10-29T12:47:21.744750Z","iopub.status.idle":"2023-10-29T12:47:26.385730Z","shell.execute_reply.started":"2023-10-29T12:47:21.744724Z","shell.execute_reply":"2023-10-29T12:47:26.384825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#文本中的全英大寫字詞出現次數。\ndef get_cap_n(txt):\n    big=0\n    txt_list = txt.split()\n    for t in txt_list:\n        if t.isupper():\n            big+=1\n    return big\n    \ndata['cap_n'] = data['clear_text_sw'].apply(lambda x: get_cap_n(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:26.387176Z","iopub.execute_input":"2023-10-29T12:47:26.387475Z","iopub.status.idle":"2023-10-29T12:47:26.448710Z","shell.execute_reply.started":"2023-10-29T12:47:26.387449Z","shell.execute_reply":"2023-10-29T12:47:26.447928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#文本中的句子總數。\ndef get_sen_n(org_txt):\n    # 會有人用多種符號來區隔句子，因此使用多種符號分割評論\n    sen_list = re.split(r';|/|\\!|\\n|\\?|\\.', org_txt)\n    # 過濾掉空字串\n    sen_list = list(filter(None, sen_list))\n    return len(sen_list)\n\n# 特別注意要使用清理前的字串作為輸入欄位\ndata['sen_n'] = data['text'].apply(lambda x: get_sen_n(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:26.449854Z","iopub.execute_input":"2023-10-29T12:47:26.450118Z","iopub.status.idle":"2023-10-29T12:47:26.555798Z","shell.execute_reply.started":"2023-10-29T12:47:26.450094Z","shell.execute_reply":"2023-10-29T12:47:26.554993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.info())","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:26.557075Z","iopub.execute_input":"2023-10-29T12:47:26.557328Z","iopub.status.idle":"2023-10-29T12:47:26.583569Z","shell.execute_reply.started":"2023-10-29T12:47:26.557305Z","shell.execute_reply":"2023-10-29T12:47:26.582698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:26.584779Z","iopub.execute_input":"2023-10-29T12:47:26.585149Z","iopub.status.idle":"2023-10-29T12:47:26.598055Z","shell.execute_reply.started":"2023-10-29T12:47:26.585115Z","shell.execute_reply":"2023-10-29T12:47:26.597163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"情感分數的數值範圍介於-1至1之間，數值趨近於-1顯示該文本平均屬於負面情感，趨近於1則顯示該文本平均屬於正面情感，若趨近於0的數值則代表情感表現不明顯，為中性情感文本。","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\ndef get_pos_sentiment_n(org_txt):\n    positive_sentiment_return=0\n    sentiment = TextBlob(org_txt)\n    for sentence in sentiment.sentences:\n        positive_sentiment_return = positive_sentiment_return+sentiment.sentiment.polarity\n    return positive_sentiment_return\n\ndef get_neg_sentiment_n(org_txt):\n    negative_sentiment_return=0\n    sentiment = TextBlob(org_txt)\n    for sentence in sentiment.sentences:\n        negative_sentiment_return = negative_sentiment_return+sentiment.sentiment.subjectivity\n    return negative_sentiment_return\n\ndata['pos_sentiment_n'] = data['text'].apply(lambda x: get_pos_sentiment_n(x))\ndata['neg_sentiment_n'] = data['text'].apply(lambda x: get_neg_sentiment_n(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:26.598995Z","iopub.execute_input":"2023-10-29T12:47:26.599261Z","iopub.status.idle":"2023-10-29T12:47:43.801590Z","shell.execute_reply.started":"2023-10-29T12:47:26.599237Z","shell.execute_reply":"2023-10-29T12:47:43.800530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:43.802766Z","iopub.execute_input":"2023-10-29T12:47:43.803130Z","iopub.status.idle":"2023-10-29T12:47:43.812581Z","shell.execute_reply.started":"2023-10-29T12:47:43.803100Z","shell.execute_reply":"2023-10-29T12:47:43.811660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport string\neng_stopwords=set(stopwords.words('english'))\n\n## Number of characters in the text ##\ndata[\"num_chars\"] = data[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ndata[\"num_stopwords\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations 標點 in the text ##\ndata[\"num_punctuations\"] =data['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ndata[\"num_words_title\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ndata[\"mean_word_len\"] = data[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:51:53.852058Z","iopub.execute_input":"2023-10-29T12:51:53.852440Z","iopub.status.idle":"2023-10-29T12:51:54.919413Z","shell.execute_reply.started":"2023-10-29T12:51:53.852413Z","shell.execute_reply":"2023-10-29T12:51:54.918332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:01:33.292455Z","iopub.execute_input":"2023-11-27T14:01:33.292887Z","iopub.status.idle":"2023-11-27T14:01:33.725063Z","shell.execute_reply.started":"2023-11-27T14:01:33.292849Z","shell.execute_reply":"2023-11-27T14:01:33.723051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data.to_csv('S_ver3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:52:51.877556Z","iopub.execute_input":"2023-10-29T12:52:51.878497Z","iopub.status.idle":"2023-10-29T12:52:52.402786Z","shell.execute_reply.started":"2023-10-29T12:52:51.878461Z","shell.execute_reply":"2023-10-29T12:52:52.401940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfrom textblob import TextBlob\n\ntext = '''The titular threat of The Blob has always struck me as the ultimate movie\nmonster: an insatiably hungry, amoeba-like mass able to penetrate\nvirtually any safeguard, capable of--as a doomed doctor chillingly\ndescribes it--assimilating flesh on contact.\nSnide comparisons to gelatin be damned, it is a concept with the most\ndevastating of potential consequences, not unlike the grey goo scenario\nproposed by technological theorists fearful of\nartificial intelligence run rampant.\n'''\n\nblob = TextBlob(text)\nprint(blob, sentence.sentiment.polarity, sentence.sentiment.subjectivity)\nfor sentence in blob.sentences:\n    print(sentence, sentence.sentiment.polarity, sentence.sentiment.subjectivity)\n    '''","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:47:44.829263Z","iopub.status.idle":"2023-10-29T12:47:44.830065Z","shell.execute_reply.started":"2023-10-29T12:47:44.829797Z","shell.execute_reply":"2023-10-29T12:47:44.829821Z"},"trusted":true},"execution_count":null,"outputs":[]}]}